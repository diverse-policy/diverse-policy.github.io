<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Diverse Policy Learning via Random Obstacle Deployment</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#abstract">Abstract</a>
      <a class="navbar-item" href="#components">Components</a>
      <a class="navbar-item" href="#experiments">Experiments</a>
      <a class="navbar-item" href="#citation">Citation</a>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title">Diverse Policy Learning via Random Obstacle Deployment for Zero-Shot Adaptation</h1>
      <p class="subtitle">IEEE Robotics and Automation Letters (RA-L), 2025</p>
      <div class="content">
        <p><strong>Authors:</strong> Seokjin Choi, Yonghyeon Lee, Seungyeon Kim, Che-Sang Park, Himchan Hwang, Frank C. Park</p>
        <div class="buttons is-centered">
          <a href="https://arxiv.org/abs/your-paper-link" class="button is-dark">
            <span class="icon"><i class="fas fa-file-pdf"></i></span>
            <span>Paper</span>
          </a>
          <a href="https://github.com/your-github-repo" class="button is-dark">
            <span class="icon"><i class="fab fa-github"></i></span>
            <span>Code on GitHub</span>
          </a>
        </div>
      </div>
    </div>
  </div>
</section>

<section id="abstract" class="section">
  <div class="container">
    <h2 class="title">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Recognizing and manipulating transparent tableware from partial view RGB image observations is made challenging 
        by the difficulty in obtaining reliable depth measurements of transparent objects. In this paper we present the 
        Transparent Tableware SuperQuadric Network (T<sup>2</sup>SQNet), a neural network model that leverages a family 
        of newly extended deformable superquadrics to produce low-dimensional, instance-wise and accurate 3D geometric 
        representations of transparent objects from partial views. As a byproduct and contribution of independent interest, 
        we also present TablewareNet, a publicly available toolset of seven parametrized shapes based on our extended 
        deformable superquadrics, that can be used to generate new datasets of tableware objects of diverse shapes and sizes. 
        Experiments with T<sup>2</sup>SQNet trained with TablewareNet show that T<sup>2</sup>SQNet outperforms existing methods
        in recognizing transparent objects, in some cases by significant margins, and can be effectively used in robotic 
        applications like decluttering and target retrieval.
      </p>
    </div>
    <div class="content has-text-centered">
      <p><strong>TL;DR:</strong> We propose a diverse policy learning framework leveraging random obstacle deployment for zero-shot adaptation in dynamic environments.</p>
    </div>
  </div>
</section>

<section id="components" class="section">
  <div class="container">
    <h2 class="title">Core Components</h2>
    <div class="content">
      <h3 class="subtitle">1. Policy</h3>
      <p>
        Our policy is designed to generate diverse actions in response to dynamically changing environments. It learns from random obstacle deployment during training to create a rich set of action strategies.
      </p>

      <h3 class="subtitle">2. Motion Predictor</h3>
      <p>
        The motion predictor forecasts state trajectories, enabling the system to evaluate the feasibility of actions in real-time. It ensures safe and effective adaptation to dynamic obstacles.
      </p>

      <h3 class="subtitle">3. Latent Skill Sampler</h3>
      <p>
        The latent skill sampler produces diverse skill variables conditioned on the current state. It leverages state-dependent distributions to enhance adaptability and action diversity.
      </p>
    </div>
  </div>
</section>

<section id="experiments" class="section">
  <div class="container">
    <h2 class="title">Experiments</h2>
    <p>We conducted extensive experiments across four reinforcement learning environments: Push-T, Ant, Reacher, and Swimmer.</p>
    <figure>
      <img src="./static/images/experiment_results.png" alt="Experiment Results">
      <figcaption>Performance comparison in unseen environments with dynamically changing obstacles.</figcaption>
    </figure>
  </div>
</section>

<section id="citation" class="section">
  <div class="container">
    <h2 class="title">Citation</h2>
    <pre><code>
@article{choi2025diverse,
  title={Diverse Policy Learning via Random Obstacle Deployment for Zero-Shot Adaptation},
  author={Choi, Seokjin and Lee, Yonghyeon and Kim, Seungyeon and Park, Che-Sang and Hwang, Himchan and Park, Frank C},
  journal={IEEE Robotics and Automation Letters},
  year={2025}
}
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="content has-text-centered">
    <p>Page created for the paper: <em>Diverse Policy Learning via Random Obstacle Deployment for Zero-Shot Adaptation</em>.</p>
  </div>
</footer>

</body>
</html>